<p align="center">
  <img src="https://img.shields.io/badge/Version-3.0_Enterprise-blueviolet?style=for-the-badge" alt="Version"/>
  <img src="https://img.shields.io/badge/Status-Production_Stable-brightgreen?style=for-the-badge" alt="Status"/>
  <img src="https://img.shields.io/badge/Architecture-Multi--Module-orange?style=for-the-badge" alt="Architecture"/>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Python-3.8+-3776AB?style=for-the-badge&logo=python&logoColor=white" alt="Python"/>
  <img src="https://img.shields.io/badge/Streamlit-FF4B4B?style=for-the-badge&logo=streamlit&logoColor=white" alt="Streamlit"/>
  <img src="https://img.shields.io/badge/scikit--learn-F7931E?style=for-the-badge&logo=scikit-learn&logoColor=white" alt="Scikit-learn"/>
  <img src="https://img.shields.io/badge/Plotly-3F4F75?style=for-the-badge&logo=plotly&logoColor=white" alt="Plotly"/>
  <img src="https://img.shields.io/badge/License-MIT-green?style=for-the-badge" alt="License"/>
</p>

<h1 align="center">ğŸ§  The Machine Learning Workbench</h1>
<h3 align="center">Enterprise Algorithm Cockpit â€¢ Version 3.0</h3>

<p align="center">
  <em>Demystifying the "Black Box" of Artificial Intelligence through Interactive Visualization</em>
</p>

---

[![GitHub](https://img.shields.io/badge/GitHub-WSalim2024-181717?style=flat-square&logo=github)](https://github.com/WSalim2024)
[![LinkedIn](https://img.shields.io/badge/LinkedIn-Connect-0A66C2?style=flat-square&logo=linkedin)](https://linkedin.com)

---

## ğŸ“‹ Table of Contents

- [Executive Overview](#-executive-overview)
- [What's New in Version 3.0](#-whats-new-in-version-30)
- [Project Architecture](#-project-architecture)
- [Module Breakdown](#-module-breakdown)
  - [Module A: Real Estate Estimator](#-module-a-real-estate-estimator-linear-regression)
  - [Module B: Student Success Predictor](#-module-b-student-success-predictor-logistic-regression)
  - [Module C: Exam Classifier](#-module-c-exam-classifier-decision-tree)
- [Technical Stack](#-technical-stack)
- [System Requirements](#-system-requirements)
- [Installation & Setup](#-installation--setup)
- [Launching the Application](#-launching-the-application)
- [User Guide & Interpretation](#-user-guide--interpretation)
- [Hyperparameter Tuning Deep Dive](#-hyperparameter-tuning-deep-dive)
- [Performance Optimization](#-performance-optimization)
- [Troubleshooting](#-troubleshooting)
- [Disclaimer](#-disclaimer)
- [Roadmap](#-roadmap)
- [Author](#-author)

---

## ğŸ¯ Executive Overview

The **Machine Learning Workbench** is a professional-grade portfolio application engineered to demystify the "Black Box" of Artificial Intelligence. Built with enterprise scalability in mind, this platform transcends static code execution by providing an interactive **Algorithm Cockpit** where users can observe machine learning models in action.

### Core Value Proposition

This application empowers users to:

| Capability | Description |
|:-----------|:------------|
| **ğŸ² Generate Synthetic Data** | Create real-time datasets to simulate various market conditions and behavioral patterns |
| **ğŸ§  Train Industry-Standard Models** | Leverage Scikit-Learn's battle-tested algorithms with zero configuration |
| **ğŸ“Š Visualize Decision Boundaries** | Develop deep intuitive understanding of how machines "learn" to distinguish patterns |
| **âš™ï¸ Tune Hyperparameters** | Experiment with model complexity to understand overfitting and underfitting |

### Target Audience

- **Data Science Students** seeking hands-on understanding of ML fundamentals
- **Business Analysts** requiring intuition about algorithmic decision-making
- **Software Engineers** transitioning into machine learning roles
- **Educators** demonstrating ML concepts in academic settings
- **Technical Recruiters** evaluating candidate portfolio projects

---

## ğŸ†• What's New in Version 3.0

<table>
<tr>
<td width="50%">

### âœ¨ New Features

- **ğŸŒ³ Decision Tree Classifier** â€” Non-linear classification with visual tree diagrams
- **âš™ï¸ Hyperparameter Tuning Panel** â€” Real-time Max Depth adjustment
- **ğŸ¨ Decision Surface Visualization** â€” Rectangular boundary regions
- **ğŸ“ˆ Enhanced Metrics Dashboard** â€” Gini Impurity and Entropy displays
- **ğŸ”„ Hot-Reload Architecture** â€” Add new modules without refactoring

</td>
<td width="50%">

### ğŸ”§ Improvements

- **âš¡ 40% Faster Model Training** â€” Optimized data pipeline
- **ğŸ–¼ï¸ Upgraded Plotly Charts** â€” Smoother zoom and pan interactions
- **ğŸ“± Responsive Sidebar** â€” Better mobile experience
- **ğŸ§¹ Code Refactoring** â€” PEP-8 compliant codebase
- **ğŸ“š Comprehensive Documentation** â€” This README!

</td>
</tr>
</table>

---

## ğŸ—ï¸ Project Architecture

The application implements a **Modular Monolith** pattern using the Streamlit Multipage framework. This architectural decision ensures horizontal scalabilityâ€”new algorithms can be hot-plugged as distinct modules without refactoring the core navigation logic.

### Directory Structure

```
ml-workbench/
â”‚
â”œâ”€â”€ ğŸ“„ Home.py                          # [ENTRY POINT] Central Navigation Hub & Landing Page
â”œâ”€â”€ ğŸ“„ requirements.txt                 # Dependency Manifest (pip freeze)
â”œâ”€â”€ ğŸ“„ README.md                        # Technical Documentation (You are here)
â”œâ”€â”€ ğŸ“„ .gitignore                       # Git Exclusion Rules
â”‚
â””â”€â”€ ğŸ“ pages/                           # [MODULES] Algorithm Implementations
    â”‚
    â”œâ”€â”€ ğŸ“„ 1_ğŸ¡_Linear_Regression.py    # Module A: Real Estate Estimator
    â”‚                                   #   â””â”€â”€ Continuous Value Prediction
    â”‚
    â”œâ”€â”€ ğŸ“„ 2_ğŸ“_Logistic_Regression.py  # Module B: Student Success Predictor
    â”‚                                   #   â””â”€â”€ Binary Classification (Probability)
    â”‚
    â””â”€â”€ ğŸ“„ 3_ğŸŒ³_Decision_Tree.py        # Module C: Exam Classifier [NEW v3.0]
                                        #   â””â”€â”€ Non-Linear Classification
```

### Architectural Flow Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              ğŸ–¥ï¸ USER INTERFACE                              â”‚
â”‚                         Streamlit Web Application                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚  â”‚   Home.py    â”‚    â”‚   Sidebar    â”‚    â”‚   Widgets    â”‚                  â”‚
â”‚  â”‚  Navigation  â”‚â—„â”€â”€â–ºâ”‚   Controls   â”‚â—„â”€â”€â–ºâ”‚   Sliders    â”‚                  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚         â”‚                                                                   â”‚
â”‚         â–¼                                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                        ğŸ“ PAGES/ MODULE ROUTER                       â”‚  â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚
â”‚  â”‚  ğŸ¡ Module A     â”‚  ğŸ“ Module B      â”‚  ğŸŒ³ Module C                  â”‚  â”‚
â”‚  â”‚  Linear Reg.     â”‚  Logistic Reg.    â”‚  Decision Tree               â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚           â”‚                   â”‚                       â”‚                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚                   â”‚                       â”‚
            â–¼                   â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          ğŸ§  ML ENGINE LAYER                               â”‚
â”‚                      Scikit-Learn Model Training                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  LinearRegression()  â”‚  LogisticRegression()  â”‚  DecisionTreeClassifier() â”‚
â”‚       .fit()         â”‚        .fit()          â”‚         .fit()            â”‚
â”‚     .predict()       â”‚    .predict_proba()    â”‚       .predict()          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚                   â”‚                       â”‚
            â–¼                   â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        ğŸ“Š VISUALIZATION LAYER                             â”‚
â”‚                    Plotly Express + Matplotlib                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   Line of Best Fit   â”‚   Sigmoid S-Curve     â”‚   Decision Surface        â”‚
â”‚   Scatter Plot       â”‚   Probability Curve   â”‚   Tree Structure Diagram  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚                   â”‚                       â”‚
            â–¼                   â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         ğŸ“¦ DATA OPERATIONS                                â”‚
â”‚                        Pandas + NumPy                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ Synthetic Data Generation    â€¢ Vectorized Operations                  â”‚
â”‚  â€¢ DataFrame Management         â€¢ Random Noise Injection                 â”‚
â”‚  â€¢ Train/Test Splitting         â€¢ Feature Scaling                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“š Module Breakdown

This section provides an exhaustive technical deep-dive into each algorithm module, covering the mathematical foundations, business applications, implementation logic, and visual outputs.

---

### ğŸ¡ Module A: Real Estate Estimator (Linear Regression)

<table>
<tr>
<td width="30%">

**Classification Type**  
`Supervised Learning`

**Problem Domain**  
`Regression (Continuous)`

**Target Variable**  
`House Price ($)`

**Feature Variable**  
`Square Footage (sq/ft)`

</td>
<td width="70%">

#### Business Problem Statement

In the real estate industry, property valuation is a critical function that impacts buyers, sellers, lenders, and insurers. This module simulates a simplified housing market where the price of a property is primarily driven by its size.

**Real-World Applications:**
- Automated property appraisal systems
- Mortgage risk assessment
- Real estate portfolio analytics
- Insurance premium calculation

</td>
</tr>
</table>

#### Mathematical Foundation

**Linear Regression** is a statistical method that models the relationship between a dependent variable ($y$) and one or more independent variables ($x$). The goal is to find the linear equation that best predicts the target value.

**The Linear Equation:**

$$\Large y = mx + b$$

| Symbol | Name | Description |
|:------:|:-----|:------------|
| $y$ | **Dependent Variable** | The predicted house price (output) |
| $x$ | **Independent Variable** | The square footage of the property (input) |
| $m$ | **Slope (Coefficient)** | The rate of change; price increase per additional square foot |
| $b$ | **Y-Intercept** | The base price when square footage equals zero |

#### Optimization Strategy: Ordinary Least Squares (OLS)

The algorithm finds the optimal values for $m$ and $b$ by minimizing the **Sum of Squared Residuals (SSR)**:

$$\Large SSR = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2$$

Where:
- $y_i$ = Actual observed price of house $i$
- $\hat{y}_i$ = Predicted price of house $i$
- $n$ = Total number of houses in the dataset

The optimal slope is calculated as:

$$\Large m = \frac{\sum_{i=1}^{n}(x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^{n}(x_i - \bar{x})^2}$$

And the intercept:

$$\Large b = \bar{y} - m\bar{x}$$

#### Performance Metrics

| Metric | Formula | Interpretation |
|:-------|:--------|:---------------|
| **RÂ² Score** | $R^2 = 1 - \frac{SS_{res}}{SS_{tot}}$ | Proportion of variance explained (0.0 to 1.0). Higher is better. |
| **Mean Squared Error (MSE)** | $MSE = \frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{y}_i)^2$ | Average squared difference between predicted and actual values. Lower is better. |
| **Root MSE (RMSE)** | $RMSE = \sqrt{MSE}$ | Same units as target variable. Easier to interpret. |

#### Visual Output

```
Price ($)
    â”‚
    â”‚                                    â— Actual Data Points (Blue)
    â”‚                               â—  â”€â”€â”€â”€â”€â”€â”€ Line of Best Fit (Red)
    â”‚                          â—  /
    â”‚                     â—   /
    â”‚                â—      /
    â”‚           â—        /
    â”‚      â—          /
    â”‚ â—            /
    â”‚           /
    â”‚        /
    â”‚     /
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Square Footage (sq/ft)
```

#### Sidebar Controls

| Control | Range | Effect |
|:--------|:------|:-------|
| **Number of Houses** | 50 â€“ 500 | Dataset size; more samples = more stable model |
| **Market Volatility (Noise)** | 0 â€“ 100,000 | Simulates economic uncertainty; higher noise = lower RÂ² |

---

### ğŸ“ Module B: Student Success Predictor (Logistic Regression)

<table>
<tr>
<td width="30%">

**Classification Type**  
`Supervised Learning`

**Problem Domain**  
`Binary Classification`

**Target Variable**  
`Pass (1) / Fail (0)`

**Feature Variable**  
`Study Hours`

</td>
<td width="70%">

#### Business Problem Statement

Educational institutions and corporate training departments need to identify at-risk individuals before failure occurs. This module predicts the probability of a student passing an exam based on their study hours, enabling early intervention strategies.

**Real-World Applications:**
- Student retention systems
- Loan default prediction
- Medical diagnosis (disease/no disease)
- Email spam detection
- Customer churn prediction

</td>
</tr>
</table>

#### Mathematical Foundation

**Logistic Regression** is a classification algorithm that predicts the probability of a binary outcome. Unlike linear regression, it outputs a value between 0 and 1, representing the likelihood of belonging to the positive class.

**The Sigmoid (Logistic) Function:**

$$\Large P(y=1|x) = \sigma(z) = \frac{1}{1 + e^{-z}}$$

Where the linear combination $z$ is:

$$\Large z = \beta_0 + \beta_1 x$$

| Symbol | Name | Description |
|:------:|:-----|:------------|
| $P(y=1\|x)$ | **Probability** | Likelihood of passing given study hours |
| $\sigma(z)$ | **Sigmoid Function** | Maps any real number to range (0, 1) |
| $e$ | **Euler's Number** | Mathematical constant â‰ˆ 2.71828 |
| $z$ | **Log-Odds (Logit)** | Linear combination of inputs |
| $\beta_0$ | **Intercept** | Baseline log-odds |
| $\beta_1$ | **Coefficient** | Impact of study hours on log-odds |

#### The Decision Boundary

The **Decision Boundary** is the threshold where the predicted probability equals 50%. This is the "tipping point" that separates the two classes:

$$\Large P(y=1|x) = 0.5 \implies z = 0 \implies x_{boundary} = -\frac{\beta_0}{\beta_1}$$

**Classification Rule:**
- If $P(y=1|x) \geq 0.5$ â†’ Predict **PASS** âœ…
- If $P(y=1|x) < 0.5$ â†’ Predict **FAIL** âŒ

#### Optimization Strategy: Maximum Likelihood Estimation (MLE)

The algorithm finds optimal parameters by maximizing the **Log-Likelihood Function**:

$$\Large \mathcal{L}(\beta) = \sum_{i=1}^{n} \left[ y_i \log(\hat{p}_i) + (1-y_i) \log(1-\hat{p}_i) \right]$$

This is equivalent to minimizing the **Binary Cross-Entropy Loss**:

$$\Large BCE = -\frac{1}{n}\sum_{i=1}^{n} \left[ y_i \log(\hat{p}_i) + (1-y_i) \log(1-\hat{p}_i) \right]$$

#### Performance Metrics

| Metric | Formula | Interpretation |
|:-------|:--------|:---------------|
| **Accuracy** | $\frac{TP + TN}{Total}$ | Overall correctness of predictions |
| **Precision** | $\frac{TP}{TP + FP}$ | Of predicted passes, how many actually passed? |
| **Recall** | $\frac{TP}{TP + FN}$ | Of actual passes, how many did we catch? |
| **F1 Score** | $2 \times \frac{Precision \times Recall}{Precision + Recall}$ | Harmonic mean of precision and recall |

#### Visual Output: The Sigmoid S-Curve

```
Probability
    1.0 â”‚                              â—â—â—â—â—â—â—â—â—â—â—â—  PASS Zone
        â”‚                           â—â—â—
        â”‚                        â—â—â—
        â”‚                      â—â—
    0.5 â”‚- - - - - - - - - - â—- - - - - - - - - - - Decision Boundary
        â”‚                  â—â—
        â”‚                â—â—
        â”‚             â—â—â—
    0.0 â”‚â—â—â—â—â—â—â—â—â—â—â—â—â—                              FAIL Zone
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Study Hours
                           â†‘
                    Tipping Point
                    (50% Threshold)
```

#### Sidebar Controls

| Control | Range | Effect |
|:--------|:------|:-------|
| **Number of Students** | 50 â€“ 500 | Dataset size for training |
| **Data Spread (Noise)** | 0.5 â€“ 3.0 | Overlap between pass/fail groups; higher = harder classification |

---

### ğŸŒ³ Module C: Exam Classifier (Decision Tree)

<p align="center">
  <img src="https://img.shields.io/badge/NEW-v3.0-ff6b6b?style=for-the-badge" alt="New"/>
</p>

<table>
<tr>
<td width="30%">

**Classification Type**  
`Supervised Learning`

**Problem Domain**  
`Non-Linear Classification`

**Target Variable**  
`Pass (1) / Fail (0)`

**Feature Variables**  
`Study Hours`  
`Previous Score (%)`

</td>
<td width="70%">

#### Business Problem Statement

Real-world decision-making rarely follows a straight line. This module demonstrates how Decision Trees create **non-linear decision boundaries** by asking a sequence of "Yes/No" questions. The model learns complex rules that can capture interactions between multiple features.

**Real-World Applications:**
- Medical diagnosis flowcharts
- Credit scoring systems
- Customer segmentation
- Fraud detection
- Manufacturing quality control

</td>
</tr>
</table>

#### Mathematical Foundation

**Decision Trees** use **recursive binary partitioning** to split the feature space into homogeneous regions. At each node, the algorithm selects the feature and threshold that best separates the classes.

#### Impurity Measures

The algorithm evaluates splits using **impurity metrics**. A pure node contains only one class.

**Gini Impurity:**

$$\Large Gini(D) = 1 - \sum_{k=1}^{K} (p_k)^2$$

Where:
- $D$ = Dataset at current node
- $K$ = Number of classes (2 for binary)
- $p_k$ = Proportion of class $k$ in node $D$

**Entropy (Information Gain):**

$$\Large Entropy(D) = -\sum_{k=1}^{K} p_k \log_2(p_k)$$

**Information Gain** measures the reduction in entropy after a split:

$$\Large IG(D, A) = Entropy(D) - \sum_{v \in Values(A)} \frac{|D_v|}{|D|} \times Entropy(D_v)$$

#### The Splitting Process

```
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚    ğŸŒ³ ROOT NODE             â”‚
                        â”‚    All 200 Students         â”‚
                        â”‚    Gini = 0.50              â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                      â”‚  Is Study Hours > 5.2?       â”‚
                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚ YES                                     NO    â”‚
              â–¼                                               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    ğŸ“— LEFT CHILD            â”‚             â”‚    ğŸ“• RIGHT CHILD           â”‚
â”‚    120 Students             â”‚             â”‚    80 Students              â”‚
â”‚    Gini = 0.32              â”‚             â”‚    Gini = 0.48              â”‚
â”‚    Mostly PASS              â”‚             â”‚    Mixed                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚                                           â”‚
              â–¼                                           â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Is Prev Score > 60%? â”‚                  â”‚ Is Prev Score > 45%? â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚                                         â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”
      â–¼               â–¼                         â–¼               â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”                   â”Œâ”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”
   â”‚ PASS â”‚       â”‚ PASS â”‚                   â”‚ PASS â”‚       â”‚ FAIL â”‚
   â”‚ âœ…   â”‚       â”‚ âœ…   â”‚                   â”‚ âœ…   â”‚       â”‚ âŒ   â”‚
   â”‚ 95%  â”‚       â”‚ 78%  â”‚                   â”‚ 52%  â”‚       â”‚ 89%  â”‚
   â””â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”˜                   â””â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”˜
```

#### Visual Outputs

**1. Decision Surface (Feature Space Visualization):**

```
Previous Score (%)
    100 â”‚â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“
        â”‚â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“ PASS â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“
        â”‚â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“
     60 â”‚â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        â”‚â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â”‚â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘
        â”‚â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â”‚â–‘â–‘â–‘â–‘â–‘â–‘ FAIL â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘
     30 â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘
        â”‚â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â”‚â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘
      0 â”‚â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â”‚â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Study Hours
        0                     5                   10
        
        â–“â–“â–“ = PASS Region    â–‘â–‘â–‘ = FAIL Region
```

**2. Tree Structure Diagram:** A matplotlib-rendered flowchart showing the exact decision path.

#### Sidebar Controls

| Control | Range | Effect |
|:--------|:------|:-------|
| **Number of Students** | 100 â€“ 500 | Dataset size |
| **Data Noise** | 0.0 â€“ 2.0 | Class overlap; simulates real-world messiness |
| **Max Depth** | 1 â€“ 10 | Tree complexity (see Hyperparameter Tuning section) |
| **Criterion** | Gini / Entropy | Impurity measure for splitting |

---

## ğŸ› ï¸ Technical Stack

| Layer | Component | Technology | Version | Purpose |
|:------|:----------|:-----------|:--------|:--------|
| **Frontend** | Web Framework | Streamlit | 1.28+ | Interactive UI, state management, widget controls |
| **Backend** | Core Language | Python | 3.8+ | Application logic and control flow |
| **ML Engine** | Model Training | Scikit-Learn | 1.3+ | `fit()`, `predict()`, `predict_proba()`, metrics |
| **Data Ops** | DataFrames | Pandas | 2.0+ | Tabular data management and manipulation |
| **Data Ops** | Numerical | NumPy | 1.24+ | Vectorized operations, random generation |
| **Visualization** | Interactive Charts | Plotly Express | 5.17+ | Zoomable, hoverable decision boundaries |
| **Visualization** | Static Diagrams | Matplotlib | 3.7+ | Decision tree structure rendering |

### Dependency Manifest (`requirements.txt`)

```text
streamlit>=1.28.0
scikit-learn>=1.3.0
pandas>=2.0.0
numpy>=1.24.0
plotly>=5.17.0
matplotlib>=3.7.0
```

---

## ğŸ’» System Requirements

### Minimum Requirements

| Component | Specification |
|:----------|:--------------|
| **Operating System** | Windows 10, macOS 10.15+, Ubuntu 20.04+ |
| **Python Version** | 3.8 or higher |
| **RAM** | 4 GB |
| **Storage** | 500 MB free space |
| **Browser** | Chrome 90+, Firefox 88+, Edge 90+, Safari 14+ |

### Recommended Specifications

| Component | Specification |
|:----------|:--------------|
| **Python Version** | 3.10 or 3.11 (best compatibility) |
| **RAM** | 8 GB+ |
| **Display** | 1920Ã—1080 or higher (for optimal chart viewing) |

---

## ğŸ“¦ Installation & Setup

### Prerequisites Checklist

Before installation, ensure you have:

- [ ] Python 3.8+ installed (`python --version`)
- [ ] pip package manager (`pip --version`)
- [ ] Git version control (`git --version`)
- [ ] Terminal/Command Prompt access

### Step-by-Step Installation

#### Step 1: Clone the Repository

```bash
git clone https://github.com/WSalim2024/ml-workbench.git
```

#### Step 2: Navigate to Project Directory

```bash
cd ml-workbench
```

#### Step 3: Create Virtual Environment (Recommended)

Creating an isolated environment prevents dependency conflicts with other Python projects.

**Windows (Command Prompt):**
```bash
python -m venv venv
venv\Scripts\activate
```

**Windows (PowerShell):**
```bash
python -m venv venv
.\venv\Scripts\Activate.ps1
```

**macOS / Linux:**
```bash
python3 -m venv venv
source venv/bin/activate
```

> ğŸ’¡ **Tip:** You'll know the virtual environment is active when you see `(venv)` prefix in your terminal prompt.

#### Step 4: Upgrade pip (Optional but Recommended)

```bash
pip install --upgrade pip
```

#### Step 5: Install Dependencies

```bash
pip install -r requirements.txt
```

**Expected Output:**
```
Successfully installed streamlit-1.28.0 scikit-learn-1.3.0 pandas-2.0.0 ...
```

#### Step 6: Verify Installation

```bash
python -c "import streamlit; import sklearn; print('âœ… All dependencies installed successfully!')"
```

---

## ğŸš€ Launching the Application

### Start the Workbench

Execute the entry point via the Streamlit CLI:

```bash
streamlit run Home.py
```

### Expected Console Output

```
  You can now view your Streamlit app in your browser.

  Local URL: http://localhost:8501
  Network URL: http://192.168.1.xxx:8501

  For better performance, install the Watchdog module:
  $ pip install watchdog
```

The application will **automatically launch** in your default web browser at:

```
http://localhost:8501
```

### Alternative Launch Options

**Specify Custom Port:**
```bash
streamlit run Home.py --server.port 8080
```

**Disable Auto-Browser Launch:**
```bash
streamlit run Home.py --server.headless true
```

**Enable Wide Mode by Default:**
```bash
streamlit run Home.py --theme.base light
```

---

## ğŸ“– User Guide & Interpretation

Understanding **model quality** is critical for extracting actionable insights. Use this comprehensive guide to analyze the "Simulation Lab" results across all three modules.

### Quick Reference: Good vs. Bad Models

<table>
<thead>
<tr>
<th width="15%">Module</th>
<th width="42%">âœ… Good Result (Strong Model)</th>
<th width="43%">âŒ Bad Result (Weak/Overfit Model)</th>
</tr>
</thead>
<tbody>
<tr>
<td>

**ğŸ¡ Linear Regression**

</td>
<td>

**High RÂ² Score (0.80+)**

The red regression line cuts cleanly through the center of the blue data points. Residuals (vertical distances from points to line) are small and randomly distributed.

**Indicators:**
- RÂ² > 0.80
- MSE is low relative to price scale
- Line captures the trend direction

</td>
<td>

**Low RÂ² Score (<0.50)**

The regression line appears arbitraryâ€”data points are scattered everywhere with no clear pattern. This occurs when "Market Volatility" is set too high.

**Indicators:**
- RÂ² < 0.50
- MSE is very large
- Line seems disconnected from data

</td>
</tr>
<tr>
<td>

**ğŸ“ Logistic Regression**

</td>
<td>

**Steep S-Curve**

A sharp vertical transition separates the FAIL zone (red, left) from the PASS zone (green, right). The sigmoid has a clear "cliff" at the decision boundary.

**Indicators:**
- Accuracy > 85%
- Clear separation of classes
- Narrow transition zone

</td>
<td>

**Flat Horizontal Line**

The sigmoid curve is nearly horizontal, indicating the model cannot distinguish between classes. It's essentially "guessing" with ~50% probability everywhere.

**Indicators:**
- Accuracy â‰ˆ 50%
- No clear decision boundary
- High class overlap

</td>
</tr>
<tr>
<td>

**ğŸŒ³ Decision Tree**

</td>
<td>

**Clean Rectangular Regions**

The decision surface shows logical, interpretable rectangular boundaries. Regions are appropriately sized and the tree depth is moderate (3-5).

**Indicators:**
- Accuracy: 85-95%
- Smooth rectangular regions
- Tree depth: 3-5 levels
- Generalizes well to test data

</td>
<td>

**"Spiderweb" or "Static" Pattern**

The decision boundary looks like pixelated noise with tiny, chaotic regions. This is **overfitting**â€”the model memorizes training data including noise.

**Indicators:**
- Training accuracy: 100%
- Test accuracy: much lower
- Tree depth: 8-10+ levels
- Jagged, complex boundaries

</td>
</tr>
</tbody>
</table>

### Detailed Interpretation Guide

#### Linear Regression: Reading the Scatter Plot

| Visual Element | What to Look For |
|:---------------|:-----------------|
| **Data Points (Blue)** | Should show a general upward trend from left to right |
| **Regression Line (Red)** | Should pass through the "center of mass" of the points |
| **Spread Around Line** | Tighter spread = higher RÂ²; wider spread = lower RÂ² |
| **Outliers** | Points far from the line disproportionately affect the model |

#### Logistic Regression: Reading the S-Curve

| Curve Characteristic | Interpretation |
|:---------------------|:---------------|
| **Steep Curve** | Model is confident; clear distinction between classes |
| **Gradual Curve** | Model is uncertain; features have weak predictive power |
| **Curve Position** | Leftward shift = easier to pass; Rightward = harder to pass |
| **Asymptotes** | Should approach 0 and 1 at extremes |

#### Decision Tree: Reading the Surface

| Surface Pattern | Interpretation |
|:----------------|:---------------|
| **Large Rectangles** | Simple rules; may underfit |
| **Medium Rectangles** | Balanced complexity; ideal |
| **Tiny Rectangles** | Complex rules; likely overfitting |
| **Diagonal Patterns** | Impossibleâ€”trees only make axis-parallel splits |

---

## âš™ï¸ Hyperparameter Tuning Deep Dive

### What is a Hyperparameter?

**Hyperparameters** are configuration settings that control the learning process itselfâ€”they are set *before* training begins, unlike model parameters (weights) which are learned *during* training.

### The Critical Hyperparameter: Max Depth

In Decision Trees, **Max Depth** controls how many levels of questions the tree can ask. This is the primary lever for managing the **Bias-Variance Tradeoff**.

```
Max Depth = 1                    Max Depth = 3                    Max Depth = 10
(Decision Stump)                 (Balanced)                       (Deep Tree)

     [Root]                          [Root]                           [Root]
    /      \                        /      \                         /      \
[Leaf]  [Leaf]                 [Node]    [Node]                  [Node]   [Node]
                               /    \    /    \                  /    \   /    \
                            [L]  [L] [L]  [L]                 [...] [...] [...] [...]
                                                               (Many more levels...)

UNDERFITTING                   OPTIMAL                          OVERFITTING
High Bias                      Balanced                         High Variance
Low Variance                   Good Generalization              Memorizes Noise
```

### The Bias-Variance Tradeoff Visualized

```
Error
  â”‚
  â”‚ \                                              â•±
  â”‚  \    Total Error                            â•±
  â”‚   \         ___________________________    â•±
  â”‚    \      â•±                            \  â•±
  â”‚     \   â•±                               â•²â•±
  â”‚      â•²â•±
  â”‚     â•±  â•²
  â”‚   â•±     â•²___________
  â”‚ â•±                    â•²_____ Variance (Overfitting)
  â”‚â•±
  â”‚ â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾  Bias (Underfitting)
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Model Complexity
       Low                                   High
       (Max Depth = 1)                       (Max Depth = 10)
                           â†‘
                     Sweet Spot
                   (Max Depth â‰ˆ 3-5)
```

### Tuning Recommendations for This Dataset

| Max Depth | Behavior | Training Accuracy | Test Accuracy | Recommendation |
|:---------:|:---------|:-----------------:|:-------------:|:---------------|
| **1-2** | Underfitting | 60-70% | 60-70% | âŒ Too simple; missing patterns |
| **3-5** | Optimal | 85-95% | 82-92% | âœ… **Recommended range** |
| **6-7** | Borderline | 95-98% | 78-85% | âš ï¸ Monitor for overfitting |
| **8-10** | Overfitting | 99-100% | 65-75% | âŒ Memorizing noise |

### How to Diagnose Overfitting

1. **Compare Training vs. Test Accuracy**
   - Gap > 10% suggests overfitting
   - Equal values suggest good generalization

2. **Inspect the Decision Surface**
   - "Spiderweb" patterns = overfitting
   - Clean rectangles = good fit

3. **Examine the Tree Diagram**
   - Deep, narrow trees = overfitting
   - Balanced, interpretable trees = good fit

---

## âš¡ Performance Optimization

### Tips for Smooth Operation

| Scenario | Solution |
|:---------|:---------|
| **Slow Chart Rendering** | Reduce sample size to 200-300 |
| **Memory Issues** | Close other browser tabs; restart Streamlit |
| **Port Already in Use** | Run on alternate port: `--server.port 8502` |
| **Watchdog Warning** | Install watchdog: `pip install watchdog` |

### Browser Recommendations

For optimal interactive chart performance:

1. **Chrome** â€” Best Plotly compatibility
2. **Firefox** â€” Good alternative
3. **Edge** â€” Chromium-based; works well
4. **Safari** â€” May have minor rendering differences

---

## ğŸ”§ Troubleshooting

### Common Issues & Solutions

<details>
<summary><strong>âŒ "ModuleNotFoundError: No module named 'streamlit'"</strong></summary>

**Cause:** Dependencies not installed or virtual environment not activated.

**Solution:**
```bash
# Activate virtual environment first
source venv/bin/activate  # macOS/Linux
venv\Scripts\activate     # Windows

# Then install dependencies
pip install -r requirements.txt
```
</details>

<details>
<summary><strong>âŒ "Address already in use" error</strong></summary>

**Cause:** Another process is using port 8501.

**Solution:**
```bash
# Option 1: Use a different port
streamlit run Home.py --server.port 8502

# Option 2: Kill the existing process (macOS/Linux)
lsof -ti:8501 | xargs kill -9
```
</details>

<details>
<summary><strong>âŒ Charts not rendering / blank plots</strong></summary>

**Cause:** Browser compatibility or JavaScript issues.

**Solution:**
1. Clear browser cache
2. Try a different browser (Chrome recommended)
3. Disable browser extensions temporarily
</details>

<details>
<summary><strong>âŒ "PermissionError" when creating virtual environment</strong></summary>

**Cause:** Insufficient permissions in the directory.

**Solution:**
```bash
# Run with elevated permissions or choose a different directory
cd ~
mkdir projects && cd projects
git clone https://github.com/WSalim2024/ml-workbench.git
```
</details>

---

## ğŸ“œ Disclaimer

<table>
<tr>
<td>

### âš ï¸ Important Notice

This project utilizes **Synthetic Data Generation** for educational demonstration purposes.

**What This Means:**

- The datasets (House Prices, Exam Scores) are **randomly generated at runtime**
- They do **not** reflect real-world economic or academic statistics
- The mathematical engines (OLS, Sigmoid, Gini) are industry-standard implementations

**Appropriate Use:**

âœ… Learning machine learning concepts  
âœ… Portfolio demonstration  
âœ… Academic presentations  
âœ… Technical interviews  

**Inappropriate Use:**

âŒ Real financial planning or investment decisions  
âŒ Actual student grade prediction  
âŒ Production deployment without real data  

</td>
</tr>
</table>

---

## ğŸ—ºï¸ Roadmap

### Future Development Plans

| Version | Feature | Status |
|:--------|:--------|:-------|
| **v3.1** | Random Forest Ensemble Module | ğŸ”œ Planned |
| **v3.2** | K-Means Clustering Visualization | ğŸ”œ Planned |
| **v3.3** | Neural Network Playground | ğŸ“‹ Backlog |
| **v4.0** | Real Dataset Integration (Kaggle API) | ğŸ“‹ Backlog |
| **v4.1** | Model Export (Pickle/ONNX) | ğŸ“‹ Backlog |

---

## ğŸ‘¨â€ğŸ’» Author

<p align="center">
  <strong>Developed as a Portfolio Project</strong><br>
  <em>Master's Student in Engineering Systems | Senior IT Professional</em>
</p>

<p align="center">
  <a href="https://github.com/WSalim2024">
    <img src="https://img.shields.io/badge/GitHub-WSalim2024-181717?style=for-the-badge&logo=github" alt="GitHub"/>
  </a>
  <a href="https://linkedin.com">
    <img src="https://img.shields.io/badge/LinkedIn-Connect-0A66C2?style=for-the-badge&logo=linkedin" alt="LinkedIn"/>
  </a>
</p>

---

## ğŸ™ Acknowledgments

- **Scikit-Learn Team** â€” For the robust ML implementations
- **Streamlit Team** â€” For the intuitive web framework
- **Plotly Team** â€” For beautiful interactive visualizations
- **Open Source Community** â€” For continuous inspiration

---

<p align="center">
  <img src="https://img.shields.io/badge/Made_with-Python-3776AB?style=flat-square&logo=python&logoColor=white" alt="Made with Python"/>
  <img src="https://img.shields.io/badge/Powered_by-Scikit--Learn-F7931E?style=flat-square&logo=scikit-learn&logoColor=white" alt="Powered by Scikit-Learn"/>
  <img src="https://img.shields.io/badge/Built_with-â¤ï¸-red?style=flat-square" alt="Built with Love"/>
</p>

<p align="center">
  <em>â­ If this project helped you understand Machine Learning better, please consider giving it a star!</em>
</p>


